{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78634d44-7e8d-4e25-893f-5d61b6a64ca7",
   "metadata": {},
   "source": [
    "In `scikit-learn`, hyperparameter tuning with parameter search (using `GridSearchCV` or `RandomizedSearchCV`) is applicable for many models. Here’s a list of commonly used models in `scikit-learn` along with parameters you might tune:\n",
    "\n",
    "### 1. **Linear Models**\n",
    "\n",
    "- **Logistic Regression** (`LogisticRegression`):\n",
    "  - `C`: Inverse of regularization strength (higher values reduce regularization).\n",
    "  - `penalty`: Regularization norm (`'l1'`, `'l2'`, `'elasticnet'`, `'none'`).\n",
    "  - `solver`: Algorithm to use in optimization (`'liblinear'`, `'saga'`, etc.).\n",
    "\n",
    "- **Linear Regression** (`LinearRegression`):\n",
    "  - Linear regression has limited tuning, but regularized forms (like Ridge and Lasso) do have hyperparameters.\n",
    "\n",
    "- **Ridge Regression** (`Ridge`):\n",
    "  - `alpha`: Regularization strength.\n",
    "  - `solver`: Optimization solver (`'auto'`, `'svd'`, `'cholesky'`, etc.).\n",
    "\n",
    "- **Lasso Regression** (`Lasso`):\n",
    "  - `alpha`: Regularization strength.\n",
    "  - `max_iter`: Maximum number of iterations.\n",
    "\n",
    "### 2. **Support Vector Machines (SVM)**\n",
    "\n",
    "- **SVC (Support Vector Classifier)** (`SVC`):\n",
    "  - `C`: Regularization parameter.\n",
    "  - `kernel`: Kernel type (`'linear'`, `'poly'`, `'rbf'`, `'sigmoid'`).\n",
    "  - `gamma`: Kernel coefficient (for `'rbf'`, `'poly'`, and `'sigmoid'`).\n",
    "\n",
    "- **SVR (Support Vector Regressor)** (`SVR`):\n",
    "  - Same as `SVC`, but applied to regression.\n",
    "\n",
    "### 3. **Decision Trees**\n",
    "\n",
    "- **DecisionTreeClassifier / DecisionTreeRegressor**:\n",
    "  - `max_depth`: Maximum depth of the tree.\n",
    "  - `min_samples_split`: Minimum number of samples to split a node.\n",
    "  - `min_samples_leaf`: Minimum number of samples required to be a leaf node.\n",
    "  - `max_features`: Number of features to consider for the best split.\n",
    "\n",
    "### 4. **Ensemble Methods**\n",
    "\n",
    "- **Random Forest (Classifier and Regressor)** (`RandomForestClassifier` / `RandomForestRegressor`):\n",
    "  - `n_estimators`: Number of trees in the forest.\n",
    "  - `max_depth`: Maximum depth of each tree.\n",
    "  - `min_samples_split`: Minimum number of samples to split an internal node.\n",
    "  - `min_samples_leaf`: Minimum number of samples required to be at a leaf node.\n",
    "  - `max_features`: Number of features to consider when looking for the best split.\n",
    "\n",
    "- **Gradient Boosting (Classifier and Regressor)** (`GradientBoostingClassifier` / `GradientBoostingRegressor`):\n",
    "  - `n_estimators`: Number of boosting stages.\n",
    "  - `learning_rate`: Step size shrinkage.\n",
    "  - `max_depth`: Maximum depth of each tree.\n",
    "  - `min_samples_split`: Minimum number of samples required to split a node.\n",
    "  - `subsample`: Fraction of samples used for each tree (can help reduce overfitting).\n",
    "\n",
    "- **XGBoost** (`XGBClassifier` / `XGBRegressor` from `xgboost` library, compatible with `scikit-learn`):\n",
    "  - `n_estimators`, `learning_rate`, `max_depth`, `subsample`, and `colsample_bytree`.\n",
    "\n",
    "### 5. **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "- **KNeighborsClassifier / KNeighborsRegressor**:\n",
    "  - `n_neighbors`: Number of neighbors.\n",
    "  - `weights`: Weight function (`'uniform'`, `'distance'`).\n",
    "  - `algorithm`: Algorithm to compute nearest neighbors (`'auto'`, `'ball_tree'`, `'kd_tree'`, `'brute'`).\n",
    "\n",
    "### 6. **Naive Bayes**\n",
    "\n",
    "Naive Bayes models have fewer tunable parameters.\n",
    "\n",
    "- **GaussianNB**: Smoothing parameter is typically used, but it's less flexible.\n",
    "- **MultinomialNB**:\n",
    "  - `alpha`: Smoothing parameter.\n",
    "\n",
    "### 7. **Clustering Algorithms**\n",
    "\n",
    "- **KMeans** (`KMeans`):\n",
    "  - `n_clusters`: Number of clusters.\n",
    "  - `init`: Initialization method (`'k-means++'`, `'random'`).\n",
    "  - `max_iter`: Maximum number of iterations.\n",
    "  - `n_init`: Number of time the algorithm will be run with different centroid seeds.\n",
    "\n",
    "- **DBSCAN** (`DBSCAN`):\n",
    "  - `eps`: Maximum distance between two samples to be considered in the same neighborhood.\n",
    "  - `min_samples`: Minimum number of samples in a neighborhood for a point to be considered a core point.\n",
    "\n",
    "### 8. **Dimensionality Reduction**\n",
    "\n",
    "- **Principal Component Analysis (PCA)** (`PCA`):\n",
    "  - `n_components`: Number of components to keep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d9e35-11a8-40fc-a561-1cdb1479a9d5",
   "metadata": {},
   "source": [
    "### Example Usage with `GridSearchCV`\n",
    "\n",
    "Here’s how you might set up a parameter search for a `RandomForestClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a5484a-0964-4e57-baaf-c092cc90eed5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mrf, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Fit the grid search to the data\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Output best parameters and score\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize model and grid search\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465ea7d9-98aa-47f8-b725-436bf35d3369",
   "metadata": {},
   "source": [
    "Parameter search with `scikit-learn` is a powerful way to fine-tune models and improve their performance. Let me know if you need specific examples for any model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cb8317-4068-4c6a-b3b9-45568b4e8779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
