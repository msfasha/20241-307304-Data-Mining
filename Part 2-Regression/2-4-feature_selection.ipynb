{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Introduction to Feature Selection**\n",
    "\n",
    "**Feature Selection** is the process of selecting a subset of relevant features (predictors) from the original dataset to improve model performance and reduce overfitting. In many machine learning tasks, especially those involving high-dimensional data, not all features are useful or necessary. Feature selection helps in reducing the complexity of the model, improving interpretability, and reducing training time.\n",
    "\n",
    "**Applications of Feature Selection**:\n",
    "- Reducing the dimensionality of the data.\n",
    "- Improving model performance by eliminating irrelevant or redundant features.\n",
    "- Enhancing model interpretability by focusing on the most important features.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Types of Feature Selection Methods**\n",
    "\n",
    "Feature selection techniques are generally classified into three categories:\n",
    "\n",
    "1. **Filter Methods**: These methods select features based on their statistical properties. They are independent of any machine learning model.\n",
    "   - Examples: Correlation Coefficient, Chi-Square Test, ANOVA, Mutual Information.\n",
    "\n",
    "2. **Wrapper Methods**: These methods evaluate feature subsets based on the performance of a specific model. Wrapper methods usually involve a search strategy (e.g., forward selection, backward elimination) and use cross-validation to assess model performance.\n",
    "   - Examples: Recursive Feature Elimination (RFE), Forward Selection, Backward Elimination.\n",
    "\n",
    "3. **Embedded Methods**: These methods select features during the model training process. They are built into the learning algorithm.\n",
    "   - Examples: Lasso (L1) Regularization, Ridge (L2) Regularization, Decision Trees, Random Forests.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Filter Methods**\n",
    "\n",
    "**Filter methods** evaluate the relevance of each feature individually and select the most relevant features based on a statistical measure. These methods are fast and computationally efficient.\n",
    "\n",
    "1. **Correlation Coefficient**: Measures the strength and direction of the linear relationship between two variables.\n",
    "   \n",
    "   - Pearson's correlation coefficient is commonly used for continuous data. Features with a high correlation with the target variable may be retained.\n",
    "\n",
    "2. **Chi-Square Test**: Used for categorical features to determine if there is a significant association between the feature and the target variable.\n",
    "   \n",
    "   $$\n",
    "   \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\n",
    "   $$\n",
    "   Where $ O_i $ is the observed frequency and $ E_i $ is the expected frequency.\n",
    "\n",
    "3. **ANOVA (Analysis of Variance)**: Used for comparing the means of different groups to find the most relevant features.\n",
    "\n",
    "4. **Mutual Information**: Measures the amount of information one feature provides about the target variable.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Wrapper Methods**\n",
    "\n",
    "**Wrapper methods** evaluate multiple feature subsets by training a machine learning model on each subset and selecting the best-performing subset.\n",
    "\n",
    "1. **Recursive Feature Elimination (RFE)**: RFE recursively removes the least important features and fits the model again until the optimal number of features is reached. This is usually done with cross-validation.\n",
    "\n",
    "2. **Forward Selection**: Starts with an empty set of features and iteratively adds features that improve the model’s performance until no significant improvement is observed.\n",
    "\n",
    "3. **Backward Elimination**: Starts with all the features and iteratively removes the least important features until no further improvement is observed.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Embedded Methods**\n",
    "\n",
    "**Embedded methods** perform feature selection during the training process. These methods are specific to certain algorithms, such as regularized linear models or tree-based methods.\n",
    "\n",
    "1. **Lasso Regression (L1 Regularization)**: Lasso adds a penalty to the absolute value of the coefficients. It tends to shrink less important feature coefficients to zero, effectively selecting a subset of features.\n",
    "\n",
    "   $$\n",
    "   L(\\beta) = \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum |\\beta_j|\n",
    "   $$\n",
    "\n",
    "2. **Ridge Regression (L2 Regularization)**: Unlike Lasso, Ridge penalizes the squared magnitude of the coefficients, making it better at handling multicollinearity but less aggressive in shrinking coefficients to zero.\n",
    "\n",
    "   $$\n",
    "   L(\\beta) = \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum \\beta_j^2\n",
    "   $$\n",
    "\n",
    "3. **Tree-Based Methods**: Decision Trees, Random Forests, and Gradient Boosting Trees can rank features based on their importance (e.g., Gini importance or Information Gain).\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Step-by-Step Example of Feature Selection**\n",
    "\n",
    "Consider a dataset where we want to predict a target variable `SalePrice` based on various features such as `Area`, `Bedrooms`, `Bathrooms`, and `GarageSize`. We will use feature selection techniques to identify the most relevant features.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **Python Code Example for Feature Selection**\n",
    "\n",
    "Here’s how to implement various feature selection techniques in Python using `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "\n",
    "# Step 1: Create the dataset\n",
    "data = {'Area': [1500, 1600, 1700, 1800, 1900],\n",
    "        'Bedrooms': [3, 4, 3, 3, 5],\n",
    "        'Bathrooms': [2, 2, 3, 2, 4],\n",
    "        'GarageSize': [1, 2, 2, 2, 3],\n",
    "        'SalePrice': [200000, 250000, 240000, 300000, 350000]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "X = df[['Area', 'Bedrooms', 'Bathrooms', 'GarageSize']]\n",
    "y = df['SalePrice']\n",
    "\n",
    "# Step 2: Filter Method - SelectKBest with F-Regression (ANOVA)\n",
    "select_k_best = SelectKBest(score_func=f_regression, k=2)  # Select top 2 features\n",
    "X_new = select_k_best.fit_transform(X, y)\n",
    "print(\"Selected features using SelectKBest (Filter):\", X.columns[select_k_best.get_support()])\n",
    "\n",
    "# Step 3: Wrapper Method - Recursive Feature Elimination (RFE)\n",
    "model = RandomForestRegressor()\n",
    "rfe = RFE(model, n_features_to_select=2)  # Select top 2 features\n",
    "rfe = rfe.fit(X, y)\n",
    "print(\"Selected features using RFE (Wrapper):\", X.columns[rfe.support_])\n",
    "\n",
    "# Step 4: Embedded Method - Lasso Regularization\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "print(\"Selected features using Lasso (Embedded):\", X.columns[lasso.coef_ != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **Step 1**: We create a small dataset with features like `Area`, `Bedrooms`, `Bathrooms`, and `GarageSize`.\n",
    "- **Step 2**: We apply **SelectKBest** to select the top 2 features based on the F-test (ANOVA).\n",
    "- **Step 3**: We use **Recursive Feature Elimination (RFE)** with a random forest model to rank the features and select the top 2.\n",
    "- **Step 4**: We apply **Lasso Regression** for feature selection. Features with non-zero coefficients are considered important.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. **Evaluating Feature Selection Methods**\n",
    "\n",
    "When selecting features, it's important to assess the impact of feature selection on model performance. This can be done using:\n",
    "\n",
    "1. **Cross-Validation**: Measure the performance of the model before and after feature selection using techniques like k-fold cross-validation.\n",
    "   \n",
    "2. **Performance Metrics**: Compare accuracy, precision, recall, F1-score (for classification), or R-squared, RMSE (for regression) after feature selection to ensure it improves model performance.\n",
    "\n",
    "3. **Computational Efficiency**: Feature selection should reduce training time and complexity, especially in high-dimensional datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### 9. **Advanced Techniques**\n",
    "\n",
    "1. **PCA (Principal Component Analysis)**: Though not technically a feature selection method, PCA is used for dimensionality reduction by transforming features into principal components.\n",
    "   \n",
    "2. **Boruta Algorithm**: A wrapper method for feature selection based on a random forest classifier that identifies relevant features.\n",
    "\n",
    "3. **SHAP Values (SHapley Additive exPlanations)**: A technique to explain the output of machine learning models by attributing each feature’s contribution to the model’s prediction.\n",
    "\n",
    "---\n",
    "\n",
    "#### 10. **Conclusion**\n",
    "\n",
    "Feature selection is a crucial step in building machine learning models, especially when dealing with high-dimensional data. It helps in improving model accuracy, reducing overfitting, and enhancing interpretability. Using methods like filter, wrapper, and embedded techniques, you can select the most relevant features for your model.\n",
    "\n",
    "**Homework**:  \n",
    "- Apply feature selection to a high-dimensional dataset using filter, wrapper, and embedded methods.\n",
    "- Experiment with different regularization techniques like Lasso and Ridge for feature selection in regression problems.\n",
    "- Try using tree-based methods (e.g., Random Forest) to rank feature importance and compare them with SelectKBest and RFE methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
