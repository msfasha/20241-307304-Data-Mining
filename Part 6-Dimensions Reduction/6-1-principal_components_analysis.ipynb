{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Introduction to Principal Component Analysis (PCA)**\n",
    "\n",
    "**Principal Component Analysis (PCA)** is an unsupervised dimensionality reduction technique used to transform high-dimensional datasets into a lower-dimensional space while retaining most of the important information (variance). The main goal of PCA is to reduce the complexity of the data while minimizing information loss.\n",
    "\n",
    "**Applications of PCA**:\n",
    "- Reducing the number of features in a dataset for machine learning tasks.\n",
    "- Visualization of high-dimensional data.\n",
    "- Removing noise and redundancy in data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **How PCA Works**\n",
    "\n",
    "PCA works by finding new dimensions called **principal components** that capture the maximum variance in the data. Each principal component is a linear combination of the original features, and they are orthogonal to each other (i.e., uncorrelated).\n",
    "\n",
    "**Steps Involved in PCA**:\n",
    "\n",
    "1. **Standardize the Data**: Ensure that the dataset has a mean of zero and unit variance for each feature.\n",
    "2. **Compute the Covariance Matrix**: The covariance matrix shows the relationships (variances and covariances) between the features in the dataset.\n",
    "3. **Compute Eigenvectors and Eigenvalues**: The eigenvectors of the covariance matrix represent the direction of the principal components, and the eigenvalues represent the magnitude of variance captured by these components.\n",
    "4. **Sort Eigenvalues and Select Principal Components**: The principal components are ordered based on the eigenvalues, with the largest eigenvalue corresponding to the first principal component (which captures the most variance).\n",
    "5. **Transform the Data**: The original data is transformed into the new space defined by the selected principal components.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Mathematical Explanation**\n",
    "\n",
    "For a given dataset $ X $ with $ n $ observations and $ m $ features:\n",
    "\n",
    "1. **Mean Center the Data**:\n",
    "   Subtract the mean from each feature to center the data around zero.\n",
    "   \n",
    "   $$\n",
    "   X_{centered} = X - \\mu\n",
    "   $$\n",
    "\n",
    "2. **Covariance Matrix**:\n",
    "   Calculate the covariance matrix $ \\Sigma $:\n",
    "   \n",
    "   $$\n",
    "   \\Sigma = \\frac{1}{n-1} X_{centered}^T X_{centered}\n",
    "   $$\n",
    "\n",
    "3. **Eigenvectors and Eigenvalues**:\n",
    "   Compute the eigenvectors $ V $ and eigenvalues $ \\lambda $ of the covariance matrix:\n",
    "   \n",
    "   $$\n",
    "   \\Sigma V = \\lambda V\n",
    "   $$\n",
    "\n",
    "4. **Select Top Principal Components**:\n",
    "   Choose the top $ k $ eigenvectors that correspond to the largest eigenvalues. These eigenvectors are the principal components.\n",
    "   \n",
    "5. **Project the Data**:\n",
    "   Transform the data into the new space using the selected principal components:\n",
    "   \n",
    "   $$\n",
    "   X_{new} = X_{centered} \\cdot V_k\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Step-by-Step Example**\n",
    "\n",
    "Letâ€™s consider a small dataset with two features:\n",
    "\n",
    "| Feature 1 | Feature 2 |\n",
    "|-----------|-----------|\n",
    "| 2.5       | 2.4       |\n",
    "| 0.5       | 0.7       |\n",
    "| 2.2       | 2.9       |\n",
    "| 1.9       | 2.2       |\n",
    "| 3.1       | 3.0       |\n",
    "| 2.3       | 2.7       |\n",
    "| 2.0       | 1.6       |\n",
    "| 1.0       | 1.1       |\n",
    "| 1.5       | 1.6       |\n",
    "| 1.1       | 0.9       |\n",
    "\n",
    "We want to reduce this dataset from two dimensions to one dimension using PCA.\n",
    "\n",
    "#### **Step 1: Standardize the Data**\n",
    "\n",
    "We first center the data by subtracting the mean from each feature:\n",
    "\n",
    "| Feature 1 (Centered) | Feature 2 (Centered) |\n",
    "|----------------------|----------------------|\n",
    "| 0.69                 | 0.49                 |\n",
    "| -1.31                | -1.21                |\n",
    "| 0.39                 | 0.99                 |\n",
    "| 0.09                 | 0.29                 |\n",
    "| 1.29                 | 1.39                 |\n",
    "| 0.49                 | 0.79                 |\n",
    "| 0.19                 | -0.31                |\n",
    "| -0.81                | -0.81                |\n",
    "| -0.31                | -0.31                |\n",
    "| -0.71                | -1.01                |\n",
    "\n",
    "#### **Step 2: Compute the Covariance Matrix**\n",
    "\n",
    "Next, we compute the covariance matrix:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "\\text{Var}(Feature 1) & \\text{Cov}(Feature 1, Feature 2) \\\\\n",
    "\\text{Cov}(Feature 1, Feature 2) & \\text{Var}(Feature 2)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "After calculation, the covariance matrix is:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "0.61655556 & 0.61544444 \\\\\n",
    "0.61544444 & 0.71655556\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Step 3: Compute Eigenvectors and Eigenvalues**\n",
    "\n",
    "Using linear algebra, we compute the eigenvalues and eigenvectors of the covariance matrix:\n",
    "\n",
    "- **Eigenvalues**: $ \\lambda_1 = 1.284 $, $ \\lambda_2 = 0.049 $\n",
    "- **Eigenvectors**:\n",
    "  $$\n",
    "  v_1 = \\begin{bmatrix} 0.67787 \\\\ 0.73518 \\end{bmatrix}, v_2 = \\begin{bmatrix} -0.73518 \\\\ 0.67787 \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "#### **Step 4: Select Principal Components**\n",
    "\n",
    "Since the first eigenvalue ($ \\lambda_1 = 1.284 $) is much larger than the second ($ \\lambda_2 = 0.049 $), the first principal component captures most of the variance. Therefore, we select the first eigenvector $ v_1 $ as our principal component.\n",
    "\n",
    "#### **Step 5: Project the Data**\n",
    "\n",
    "We project the centered data onto the first principal component:\n",
    "\n",
    "$$\n",
    "X_{new} = X_{centered} \\cdot v_1\n",
    "$$\n",
    "\n",
    "This gives us the data in the reduced one-dimensional space.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Python Code Example**\n",
    "\n",
    "Here's how you can implement PCA using Python's `scikit-learn` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    Feature 1  Feature 2\n",
      "0        2.5        2.4\n",
      "1        0.5        0.7\n",
      "2        2.2        2.9\n",
      "3        1.9        2.2\n",
      "4        3.1        3.0\n",
      "5        2.3        2.7\n",
      "6        2.0        1.6\n",
      "7        1.0        1.1\n",
      "8        1.5        1.6\n",
      "9        1.1        0.9\n",
      "\n",
      "Transformed Data (1 Principal Component):\n",
      " [[ 1.08643242]\n",
      " [-2.3089372 ]\n",
      " [ 1.24191895]\n",
      " [ 0.34078247]\n",
      " [ 2.18429003]\n",
      " [ 1.16073946]\n",
      " [-0.09260467]\n",
      " [-1.48210777]\n",
      " [-0.56722643]\n",
      " [-1.56328726]]\n",
      "\n",
      "Explained Variance Ratio: [0.96296464]\n",
      "\n",
      "Reconstructed Data:\n",
      " [[ 0.76822373  0.76822373]\n",
      " [-1.63266515 -1.63266515]\n",
      " [ 0.87816931  0.87816931]\n",
      " [ 0.2409696   0.2409696 ]\n",
      " [ 1.54452629  1.54452629]\n",
      " [ 0.82076675  0.82076675]\n",
      " [-0.06548139 -0.06548139]\n",
      " [-1.04800846 -1.04800846]\n",
      " [-0.40108966 -0.40108966]\n",
      " [-1.10541102 -1.10541102]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Create a dataset\n",
    "data = {'Feature 1': [2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1],\n",
    "        'Feature 2': [2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Step 3: Apply PCA\n",
    "pca = PCA(n_components=1)  # Reduce to 1 dimension\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 4: Results\n",
    "print(\"Original Data:\\n\", df)\n",
    "print(\"\\nTransformed Data (1 Principal Component):\\n\", X_pca)\n",
    "print(\"\\nExplained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Step 5: Optional - Transform back to original space to see data variance captured\n",
    "X_projected_back = pca.inverse_transform(X_pca)\n",
    "print(\"\\nReconstructed Data:\\n\", X_projected_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **Step 1**: We create a simple dataset with two features.\n",
    "- **Step 2**: We standardize the data so that it has a mean of 0 and variance of 1.\n",
    "- **Step 3**: We apply PCA to reduce the data to one principal component.\n",
    "- **Step 4**: We print the transformed data (now in 1 dimension) and the amount of variance captured by this principal component.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. **Conclusion**\n",
    "\n",
    "PCA is a powerful technique for reducing the dimensionality of datasets, which is especially useful in machine learning when dealing with high-dimensional data. It helps simplify models, reduces computational complexity, and improves model performance by focusing on the most important features that capture the most variance.\n",
    "\n",
    "**Homework**:  \n",
    "Apply PCA to a dataset with more than 3 dimensions, reduce it to 2 dimensions, and visualize the data in the new 2D space. Analyze how much variance is captured by the first two principal components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
