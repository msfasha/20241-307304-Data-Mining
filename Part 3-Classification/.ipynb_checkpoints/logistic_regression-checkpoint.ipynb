{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Introduction to Logistic Regression**\n",
    "\n",
    "**Logistic regression** is a supervised learning algorithm used for binary classification problems, where the target variable has two possible outcomes (e.g., \"yes\" or \"no,\" \"0\" or \"1\").<br>\n",
    "Unlike linear regression, logistic regression predicts the probability that an observation belongs to a certain class using a logistic function (also called the sigmoid function).\n",
    "\n",
    "**Examples of Logistic Regression Use Cases**:\n",
    "- Predicting whether an email is spam or not spam.\n",
    "- Determining if a customer will buy a product (yes/no).\n",
    "- Classifying if a patient has a disease (positive/negative).\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Logistic Function (Sigmoid Function)**\n",
    "\n",
    "The logistic function is used to map the predicted values to a probability between 0 and 1. It has the following form:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n $\n",
    "- $ \\beta_0 $ is the intercept, $ \\beta_1, \\beta_2, ..., \\beta_n $ are the coefficients, and $ X_1, X_2, ..., X_n $ are the feature values.\n",
    "\n",
    "The output of the sigmoid function is interpreted as the probability that the observation belongs to the positive class (1). For example, if $ \\sigma(z) = 0.8 $, it means there is an 80% chance that the observation belongs to the positive class.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Decision Boundary**\n",
    "\n",
    "In logistic regression, a **decision boundary** is established at 0.5 probability. If the predicted probability $ P(y=1) $ is greater than 0.5, the model classifies the observation as belonging to class 1. If the probability is less than 0.5, the observation is classified as class 0.\n",
    "\n",
    "$$\n",
    "P(y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n",
    "$$\n",
    "Thus, the predicted class $ \\hat{y} $ is:\n",
    "- $ \\hat{y} = 1 $, if $ P(y=1) > 0.5 $\n",
    "- $ \\hat{y} = 0 $, if $ P(y=1) \\leq 0.5 $\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Log-Loss (Cost Function)**\n",
    "\n",
    "The cost function in logistic regression is **log-loss** (or **binary cross-entropy**). It penalizes incorrect classifications, especially those with high confidence.\n",
    "\n",
    "$$\n",
    "Cost(h_\\theta(x), y) = -[y \\log(h_\\theta(x)) + (1 - y) \\log(1 - h_\\theta(x))]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h_\\theta(x) $ is the predicted probability for class 1.\n",
    "- $ y $ is the actual label (0 or 1).\n",
    "\n",
    "Logistic regression tries to minimize this cost function to find the best-fit parameters (coefficients).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Step-by-Step Example**\n",
    "\n",
    "Consider a dataset where we want to predict whether a person will buy a product (Buy = 1) or not (Buy = 0) based on their age and income.\n",
    "\n",
    "| Age  | Income  | Buy (y) |\n",
    "|------|---------|---------|\n",
    "| 25   | 50000   | 0       |\n",
    "| 45   | 100000  | 1       |\n",
    "| 35   | 75000   | 1       |\n",
    "| 30   | 60000   | 0       |\n",
    "| 50   | 120000  | 1       |\n",
    "\n",
    "We’ll use this small dataset to demonstrate how logistic regression works, focusing on calculating the probabilities, making predictions, and understanding the decision boundary.\n",
    "\n",
    "### **Step-by-Step Process**\n",
    "\n",
    "#### **Step 1: Hypothesis Function**\n",
    "Logistic regression models the probability that \\( y = 1 \\) (i.e., the person buys the product) using the logistic function:\n",
    "\n",
    "$$\n",
    "P(y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\beta_0 $ is the intercept,\n",
    "- $ \\beta_1 $ and $ \\beta_2 $ are the coefficients for age and income,\n",
    "- $ X_1 $ and $ X_2 $ represent the values for age and income, respectively.\n",
    "\n",
    "#### **Step 2: Train the Model**\n",
    "\n",
    "The logistic regression model will find the best values for \\( \\beta_0 \\), \\( \\beta_1 \\), and \\( \\beta_2 \\) by minimizing the cost function (log-loss).\n",
    "\n",
    "For this small dataset, the model might output something like:\n",
    "\n",
    "$$\n",
    "P(y = 1 | X) = \\frac{1}{1 + e^{-(2 + 0.04 \\cdot Age + 0.00005 \\cdot Income)}}\n",
    "$$\n",
    "\n",
    "These numbers (2, 0.04, 0.00005) are the coefficients and intercept estimated by the model.\n",
    "\n",
    "#### **Step 3: Predict Probabilities**\n",
    "\n",
    "Let’s calculate the probability that a 35-year-old person with an income of 75,000 will buy the product:\n",
    "\n",
    "$$\n",
    "P(y = 1 | X) = \\frac{1}{1 + e^{-(2 + 0.04 \\cdot 35 + 0.00005 \\cdot 75000)}}\n",
    "$$\n",
    "First, calculate the linear combination:\n",
    "\n",
    "$$\n",
    "z = 2 + (0.04 \\cdot 35) + (0.00005 \\cdot 75000) = 2 + 1.4 + 3.75 = 7.15\n",
    "$$\n",
    "\n",
    "Now apply the logistic function:\n",
    "\n",
    "$$\n",
    "P(y = 1 | X) = \\frac{1}{1 + e^{-7.15}} \\approx \\frac{1}{1 + 0.0008} \\approx 0.9992\n",
    "$$\n",
    "\n",
    "The model predicts that there is a 99.92% chance that this person will buy the product.\n",
    "\n",
    "#### **Step 4: Make the Final Prediction**\n",
    "\n",
    "Based on the calculated probability, we apply the decision boundary (usually 0.5). Since 0.9992 is greater than 0.5, the model predicts **Class 1** (the person will buy the product).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. **Python Code Example**\n",
    "\n",
    "Here's how you can implement logistic regression using Python and the `scikit-learn` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Predicted class for new observation: 1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Create a dataset\n",
    "data = {'Age': [25, 45, 35, 30, 50],\n",
    "        'Income': [50000, 100000, 75000, 60000, 120000],\n",
    "        'Buy': [0, 1, 1, 0, 1]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Define features (Age, Income) and target (Buy)\n",
    "X = df[['Age', 'Income']]  # Features\n",
    "y = df['Buy']  # Target variable\n",
    "\n",
    "# Step 3: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Step 5: Train the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Step 8: Predict for a new observation (Age = 40, Income = 85000)\n",
    "new_data = pd.DataFrame({'Age': [40], 'Income': [85000]})\n",
    "prediction = log_reg.predict(new_data)\n",
    "print(f'Predicted class for new observation: {prediction[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **Step 1**: We create a dataset with age, income, and whether the person buys the product.\n",
    "- **Step 2**: We define features (age and income) and the target variable (buy).\n",
    "- **Step 3**: The dataset is split into training and testing sets.\n",
    "- **Step 4**: We initialize the logistic regression model.\n",
    "- **Step 5**: We train the model on the training set.\n",
    "- **Step 6**: We use the trained model to make predictions on the test set.\n",
    "- **Step 7**: We evaluate the accuracy of the model.\n",
    "- **Step 8**: We predict the probability for a new data point (Age = 40, Income = 85000).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. **Conclusion**\n",
    "\n",
    "Logistic regression is a widely used algorithm for binary classification tasks. By modeling the probability of a binary outcome using the logistic function, it provides a powerful and interpretable solution to classification problems. \n",
    "\n",
    "**Homework**:  \n",
    "Train a logistic regression model on a larger dataset with multiple features, and analyze the results. Try adjusting the threshold from 0.5 to 0.6 or 0.7 and observe how it affects the classification results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
