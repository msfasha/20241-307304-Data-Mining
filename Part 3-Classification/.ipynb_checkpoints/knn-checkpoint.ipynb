{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Introduction to Classification**\n",
    "\n",
    "**Classification** is a supervised machine learning technique where the goal is to predict the categorical label of an observation based on the features (predictors). The model is trained using labeled data, where the target (output) is categorical, such as classifying emails as \"spam\" or \"not spam,\" or predicting whether a tumor is \"malignant\" or \"benign.\"\n",
    "\n",
    "**Key Terms**:\n",
    "- **Training Set**: The dataset used to train the model.\n",
    "- **Test Set**: The dataset used to evaluate the model's performance.\n",
    "- **Target Variable**: The categorical variable the model tries to predict.\n",
    "\n",
    "Examples of classification problems:\n",
    "- Email filtering (spam vs. not spam)\n",
    "- Image recognition (dog vs. cat)\n",
    "- Credit scoring (good vs. bad)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **K-Nearest Neighbors (KNN) Algorithm**\n",
    "\n",
    "The **K-Nearest Neighbors (KNN)** algorithm is a simple, intuitive classification algorithm. It classifies new data points based on the class of the *K* nearest points in the training dataset.\n",
    "\n",
    "**How KNN Works**:\n",
    "1. **Choose K**: The number of nearest neighbors (K) to consider.\n",
    "2. **Calculate Distance**: For a given data point, calculate the distance between this point and all points in the training dataset. The most common distance metric is **Euclidean distance**.\n",
    "3. **Select Neighbors**: Select the K nearest neighbors (the ones with the smallest distance).\n",
    "4. **Vote**: The new data point is classified based on the majority class of its nearest neighbors.\n",
    "\n",
    "#### Euclidean Distance Formula:\n",
    "\n",
    "For two points $ x_1 $, $ y_1 $ and $ x_2 $, $ y_2 $, the Euclidean distance is:\n",
    "\n",
    "$$\n",
    "d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n",
    "$$\n",
    "\n",
    "#### 3. **Step-by-Step Example**\n",
    "\n",
    "Let’s use a simple dataset with two features (height and weight) and two classes (Class A and Class B).\n",
    "\n",
    "| Height | Weight | Class  |\n",
    "|--------|--------|--------|\n",
    "| 5.1    | 120    | A      |\n",
    "| 5.5    | 150    | A      |\n",
    "| 5.0    | 130    | A      |\n",
    "| 6.0    | 170    | B      |\n",
    "| 6.2    | 180    | B      |\n",
    "| 5.9    | 160    | B      |\n",
    "\n",
    "We will classify a new data point: **Height = 5.4, Weight = 140** using KNN with \\( K = 3 \\).\n",
    "\n",
    "- **Step 1**: Calculate the Euclidean distance between the new data point and all other points in the dataset.\n",
    "\n",
    "For example:\n",
    "$$\n",
    "d = \\sqrt{(5.4 - 5.1)^2 + (140 - 120)^2} ≈ 20.002\n",
    "$$\n",
    "We would repeat this for all points.\n",
    "\n",
    "- **Step 2**: Sort the distances and pick the 3 closest neighbors.\n",
    "\n",
    "Let’s assume the 3 nearest neighbors are:\n",
    "- (5.1, 120) → Class A\n",
    "- (5.0, 130) → Class A\n",
    "- (5.5, 150) → Class A\n",
    "\n",
    "- **Step 3**: Vote on the class of the new point. In this case, since all three nearest neighbors are in Class A, the new point would be classified as **Class A**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Python Code Example**\n",
    "\n",
    "Here's an implementation of KNN using Python's `scikit-learn` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00\n",
      "Predicted class for new data point: B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\me\\anaconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Create a simple dataset\n",
    "data = {'Height': [5.1, 5.5, 5.0, 6.0, 6.2, 5.9],\n",
    "        'Weight': [120, 150, 130, 170, 180, 160],\n",
    "        'Class': ['A', 'A', 'A', 'B', 'B', 'B']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Define features (Height, Weight) and target (Class)\n",
    "X = df[['Height', 'Weight']]  # Features\n",
    "y = df['Class']  # Target\n",
    "\n",
    "# Step 3: Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Initialize the KNN classifier with K=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Step 5: Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Step 8: Classify a new data point (Height = 5.4, Weight = 140)\n",
    "new_data = np.array([[5.4, 140]])\n",
    "prediction = knn.predict(new_data)\n",
    "print(f'Predicted class for new data point: {prediction[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- **Step 1**: We create a small dataset with height, weight, and class.\n",
    "- **Step 2**: The features (X) are height and weight, while the target (y) is the class.\n",
    "- **Step 3**: We split the dataset into training and testing sets.\n",
    "- **Step 4**: We initialize a KNN classifier with \\( K = 3 \\).\n",
    "- **Step 5**: We train the model using the training set.\n",
    "- **Step 6**: We use the trained model to make predictions on the test set.\n",
    "- **Step 7**: We evaluate the model's accuracy.\n",
    "- **Step 8**: We classify a new data point based on its height and weight.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Conclusion**\n",
    "\n",
    "The KNN algorithm is a simple yet powerful classification tool that is easy to understand and implement. It’s useful for problems where the relationship between features is complex, as it makes no assumptions about the underlying data distribution. However, KNN can be computationally expensive for large datasets, as the algorithm requires calculating the distance between every data point.\n",
    "\n",
    "**Homework**:  \n",
    "Using the same dataset, try different values of \\( K \\) and observe how the accuracy changes. Additionally, experiment with a larger dataset and analyze the trade-off between accuracy and computational cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
